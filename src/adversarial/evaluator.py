"""Adversarial Evaluator - Main Integration

Combines fuzz testing, adversarial generation, and mutation testing
into a unified evaluation framework.
"""

import logging
from dataclasses import dataclass
from typing import Dict, Any, List, Optional

from .fuzz_tester import FuzzTester, FuzzResult
from .adversarial_generator import AdversarialGenerator, AdversarialTest
from .mutation_tester import PatchMutationTester, MutationResult

logger = logging.getLogger(__name__)


@dataclass
class AdversarialScore:
    """Complete adversarial evaluation score"""
    # Component scores (0-1)
    fuzz_score: float
    adversarial_score: float
    mutation_score: float
    
    # Overall score
    overall_score: float
    
    # Pass/fail determination
    passed: bool
    
    # Detailed results
    fuzz_result: Optional[FuzzResult] = None
    adversarial_result: Optional[Dict[str, Any]] = None
    mutation_result: Optional[MutationResult] = None
    
    # Flags
    high_risk_issues: List[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "fuzz_score": self.fuzz_score,
            "adversarial_score": self.adversarial_score,
            "mutation_score": self.mutation_score,
            "overall_score": self.overall_score,
            "passed": self.passed,
            "high_risk_issues": self.high_risk_issues or [],
            "fuzz_details": {
                "total": self.fuzz_result.total_tests if self.fuzz_result else 0,
                "passed": self.fuzz_result.passed if self.fuzz_result else 0,
                "crashes": self.fuzz_result.crashes if self.fuzz_result else 0,
            } if self.fuzz_result else None,
            "mutation_details": {
                "total": self.mutation_result.total_mutants if self.mutation_result else 0,
                "killed": self.mutation_result.killed if self.mutation_result else 0,
                "survived": self.mutation_result.survived if self.mutation_result else 0,
            } if self.mutation_result else None,
        }


class AdversarialEvaluator:
    """
    Main adversarial evaluation framework.
    
    Integrates:
    - Fuzz testing (property-based tests)
    - Adversarial input generation
    - Patch mutation testing
    """
    
    def __init__(
        self,
        llm_client=None,
        seed: Optional[int] = None,
        weights: Optional[Dict[str, float]] = None
    ):
        self.fuzz_tester = FuzzTester(seed=seed)
        self.adversarial_gen = AdversarialGenerator(client=llm_client)
        self.mutation_tester = PatchMutationTester(seed=seed)
        
        # Score weights
        self.weights = weights or {
            "fuzz": 0.3,
            "adversarial": 0.3,
            "mutation": 0.4
        }
        
        self.stats = {
            "evaluations": 0,
            "passed": 0,
            "failed": 0,
            "high_risk": 0,
        }
    
    async def evaluate(
        self,
        instance: Dict[str, Any],
        generated_patch: str,
        expected_patch: Optional[str] = None,
        run_fuzz: bool = True,
        run_adversarial: bool = True,
        run_mutation: bool = True
    ) -> AdversarialScore:
        """
        Run full adversarial evaluation on a generated patch.
        
        Args:
            instance: SWE-bench instance
            generated_patch: Patch generated by the agent
            expected_patch: Expected/gold patch (for mutation testing)
            run_fuzz: Whether to run fuzz testing
            run_adversarial: Whether to run adversarial testing
            run_mutation: Whether to run mutation testing
            
        Returns:
            AdversarialScore with results
        """
        self.stats["evaluations"] += 1
        
        problem_statement = instance.get("problem_statement", "")
        high_risk_issues = []
        
        # Run fuzz testing
        fuzz_result = None
        fuzz_score = 1.0
        if run_fuzz:
            fuzz_result = self.fuzz_tester.run_fuzz_tests(
                generated_patch, problem_statement
            )
            fuzz_score = fuzz_result.score
            
            if fuzz_result.crashes > 0:
                high_risk_issues.append(
                    f"Fuzz testing found {fuzz_result.crashes} potential crashes"
                )
        
        # Run adversarial testing
        adversarial_result = None
        adversarial_score = 1.0
        if run_adversarial:
            adversarial_tests = await self.adversarial_gen.generate_edge_cases(
                problem_statement, generated_patch, num_cases=5
            )
            adversarial_result = self.adversarial_gen.evaluate_patch_against_tests(
                generated_patch, adversarial_tests
            )
            adversarial_score = adversarial_result["score"]
            
            if adversarial_result["likely_vulnerable"] > 0:
                high_risk_issues.append(
                    f"Patch may be vulnerable to {adversarial_result['likely_vulnerable']} attack vectors"
                )
        
        # Run mutation testing
        mutation_result = None
        mutation_score = 1.0
        if run_mutation and expected_patch:
            mutation_result = self.mutation_tester.run_mutation_testing(
                generated_patch, expected_patch
            )
            mutation_score = mutation_result.score
            
            if mutation_result.survived > mutation_result.killed:
                high_risk_issues.append(
                    f"Low mutation score: {mutation_score:.0%} - patch may be fragile"
                )
        
        # Calculate overall score
        overall_score = (
            self.weights["fuzz"] * fuzz_score +
            self.weights["adversarial"] * adversarial_score +
            self.weights["mutation"] * mutation_score
        )
        
        # Determine pass/fail
        passed = (
            overall_score >= 0.5 and
            fuzz_score >= 0.3 and
            len(high_risk_issues) < 3
        )
        
        if passed:
            self.stats["passed"] += 1
        else:
            self.stats["failed"] += 1
        
        if high_risk_issues:
            self.stats["high_risk"] += 1
        
        return AdversarialScore(
            fuzz_score=fuzz_score,
            adversarial_score=adversarial_score,
            mutation_score=mutation_score,
            overall_score=overall_score,
            passed=passed,
            fuzz_result=fuzz_result,
            adversarial_result=adversarial_result,
            mutation_result=mutation_result,
            high_risk_issues=high_risk_issues
        )
    
    async def evaluate_batch(
        self,
        instances: List[Dict[str, Any]],
        patches: List[str],
        expected_patches: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate multiple patches.
        
        Returns aggregate statistics.
        """
        results = []
        
        for i, (instance, patch) in enumerate(zip(instances, patches)):
            expected = expected_patches[i] if expected_patches else None
            
            score = await self.evaluate(instance, patch, expected)
            results.append(score)
            
            logger.info(
                f"[{i+1}/{len(instances)}] {instance.get('instance_id', 'unknown')}: "
                f"overall={score.overall_score:.2f}, passed={score.passed}"
            )
        
        # Aggregate statistics
        total = len(results)
        passed = sum(1 for r in results if r.passed)
        
        avg_fuzz = sum(r.fuzz_score for r in results) / total if total > 0 else 0
        avg_adversarial = sum(r.adversarial_score for r in results) / total if total > 0 else 0
        avg_mutation = sum(r.mutation_score for r in results) / total if total > 0 else 0
        avg_overall = sum(r.overall_score for r in results) / total if total > 0 else 0
        
        return {
            "total": total,
            "passed": passed,
            "failed": total - passed,
            "pass_rate": passed / total if total > 0 else 0,
            "avg_fuzz_score": avg_fuzz,
            "avg_adversarial_score": avg_adversarial,
            "avg_mutation_score": avg_mutation,
            "avg_overall_score": avg_overall,
            "results": [r.to_dict() for r in results]
        }
    
    def quick_evaluate(
        self,
        generated_patch: str,
        problem_statement: str = ""
    ) -> Dict[str, Any]:
        """
        Quick synchronous evaluation (no LLM, no mutation).
        
        For fast screening of patches.
        """
        # Fuzz test only
        fuzz_result = self.fuzz_tester.run_fuzz_tests(
            generated_patch, problem_statement, num_random_tests=10
        )
        
        # Heuristic adversarial
        adversarial_tests = self.adversarial_gen._generate_heuristic(
            problem_statement, generated_patch, 3
        )
        adversarial_result = self.adversarial_gen.evaluate_patch_against_tests(
            generated_patch, adversarial_tests
        )
        
        return {
            "fuzz_score": fuzz_result.score,
            "adversarial_score": adversarial_result["score"],
            "quick_score": (fuzz_result.score + adversarial_result["score"]) / 2,
            "crashes": fuzz_result.crashes,
            "vulnerabilities": adversarial_result["likely_vulnerable"],
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get evaluator statistics"""
        return {
            **self.stats,
            "pass_rate": (
                self.stats["passed"] / self.stats["evaluations"]
                if self.stats["evaluations"] > 0 else 0
            ),
            "fuzz_stats": self.fuzz_tester.get_statistics(),
            "adversarial_stats": self.adversarial_gen.get_statistics(),
            "mutation_stats": self.mutation_tester.get_statistics(),
        }
